{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f76b9e",
   "metadata": {},
   "source": [
    "# Final Ensemble Notebook\n",
    "\n",
    "Goal: build a strong word + character TF‑IDF ensemble for the news classification task, evaluate it on a validation split, then train on all data and generate submission files for the competition.\n",
    "\n",
    "High‑level steps:\n",
    "- Construct a rich text field from source, title, and article.\n",
    "- Define word‑level and character‑level TF‑IDF + Logistic Regression pipelines.\n",
    "- Compare individual models and a weighted ensemble on a validation set.\n",
    "- Retrain the best ensemble on all labeled data.\n",
    "- Apply the ensemble to `evaluation.csv` and save submission CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fe124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 63997 Val: 16000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_text(df):\n",
    "    src = df[\"source\"].fillna(\"\")\n",
    "    title = df[\"title\"].fillna(\"\")\n",
    "    art = df[\"article\"].fillna(\"\")\n",
    "    return (src + \" \") * 2 + title + \" \" + art   # <- source x2\n",
    "\n",
    "train_df = pd.read_csv(\"development.csv\")\n",
    "train_df[\"text\"] = build_text(train_df)\n",
    "\n",
    "X = train_df[\"text\"]\n",
    "y = train_df[\"label\"]\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train_text), \"Val:\", len(X_val_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ce54e",
   "metadata": {},
   "source": [
    "### Step 1 – Build text and create train/validation split\n",
    "\n",
    "**Aim:** Combine `source`, `title`, and `article` into a single text field (with `source` repeated twice for extra weight), then create a stratified train/validation split.\n",
    "\n",
    "- `build_text` defines how raw columns are merged into `text`.\n",
    "- `train_df` loads `development.csv` and adds the `text` column.\n",
    "- `train_test_split(..., stratify=y)` ensures label proportions are similar in train and validation.\n",
    "\n",
    "**What the results show:**\n",
    "- The printed `Train: ... Val: ...` line confirms the data loaded correctly and that the split sizes look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e5270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "word_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=3,\n",
    "        sublinear_tf=True,\n",
    "        max_features=100000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        C=2.0,\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ffe09",
   "metadata": {},
   "source": [
    "### Step 2 – Define word‑level TF‑IDF + Logistic Regression pipeline\n",
    "\n",
    "**Aim:** Create the main word‑based model that converts text into word n‑gram TF‑IDF features and trains a Logistic Regression classifier.\n",
    "\n",
    "- `TfidfVectorizer` uses word n‑grams `(1, 3)`, ignores English stop words, applies `min_df=3`, and caps features at 100k with `sublinear_tf=True`.\n",
    "- `LogisticRegression` uses `C=2.0`, `max_iter=2000`, and `class_weight='balanced'` to handle class imbalance and high dimensionality.\n",
    "\n",
    "**What the results show:**\n",
    "- No printed output yet; this cell just defines `word_pipeline` to be fitted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57807822",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=3,\n",
    "        sublinear_tf=True,\n",
    "        max_features=200000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        C=2.0,\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb8e18ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(2, 4) -> macroF1=0.70950\n",
      "ngram_range=(3, 5) -> macroF1=0.71417\n",
      "ngram_range=(4, 6) -> macroF1=0.71376\n",
      "Best char ngram_range: (3, 5) with macroF1= 0.71417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 4.a) Tune char ngram_range\n",
    "char_ngram_grid = [(2, 4), (3, 5), (4, 6)]\n",
    "best_ng = None\n",
    "best_f1 = -1.0\n",
    "\n",
    "for ng in char_ngram_grid:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=ng,\n",
    "            min_df=3,\n",
    "            sublinear_tf=True,\n",
    "            max_features=200000,\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            C=2.0,\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        )),\n",
    "    ])\n",
    "    pipe.fit(X_train_text, y_train)\n",
    "    pred = pipe.predict(X_val_text)\n",
    "    f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    print(f\"ngram_range={ng} -> macroF1={f1:.5f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_ng = ng\n",
    "\n",
    "print(\"Best char ngram_range:\", best_ng, \"with macroF1=\", round(best_f1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "527fbaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df=2 -> macroF1=0.71335\n",
      "min_df=3 -> macroF1=0.71417\n",
      "min_df=4 -> macroF1=0.71394\n",
      "min_df=5 -> macroF1=0.71315\n",
      "Best char min_df: 3 with macroF1= 0.71417\n"
     ]
    }
   ],
   "source": [
    "# 4.b) Tune char min_df\n",
    "char_min_df_grid = [2, 3, 4, 5]\n",
    "best_min_df = None\n",
    "best_f1_md = -1.0\n",
    "\n",
    "for md in char_min_df_grid:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=md,\n",
    "            sublinear_tf=True,\n",
    "            max_features=200000,\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            C=2.0,\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        )),\n",
    "    ])\n",
    "    pipe.fit(X_train_text, y_train)\n",
    "    pred = pipe.predict(X_val_text)\n",
    "    f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    print(f\"min_df={md} -> macroF1={f1:.5f}\")\n",
    "    if f1 > best_f1_md:\n",
    "        best_f1_md = f1\n",
    "        best_min_df = md\n",
    "\n",
    "print(\"Best char min_df:\", best_min_df, \"with macroF1=\", round(best_f1_md, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d199c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=50000 -> macroF1=0.71195\n",
      "max_features=100000 -> macroF1=0.71326\n",
      "max_features=200000 -> macroF1=0.71417\n",
      "max_features=None -> macroF1=0.71374\n",
      "Best char max_features: 200000 with macroF1= 0.71417\n"
     ]
    }
   ],
   "source": [
    "# 4.c) Tune char max_features\n",
    "char_max_feat_grid = [50000, 100000, 200000, None]\n",
    "best_max_feat = None\n",
    "best_f1_mf = -1.0\n",
    "\n",
    "for mf in char_max_feat_grid:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=3,\n",
    "            sublinear_tf=True,\n",
    "            max_features=mf,\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            C=2.0,\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        )),\n",
    "    ])\n",
    "    pipe.fit(X_train_text, y_train)\n",
    "    pred = pipe.predict(X_val_text)\n",
    "    f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    print(f\"max_features={mf} -> macroF1={f1:.5f}\")\n",
    "    if f1 > best_f1_mf:\n",
    "        best_f1_mf = f1\n",
    "        best_max_feat = mf\n",
    "\n",
    "print(\"Best char max_features:\", best_max_feat, \"with macroF1=\", round(best_f1_mf, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a61179cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.5 -> macroF1=0.70617\n",
      "C=1.0 -> macroF1=0.71001\n",
      "C=2.0 -> macroF1=0.71417\n",
      "C=4.0 -> macroF1=0.71046\n",
      "Best char C: 2.0 with macroF1= 0.71417\n"
     ]
    }
   ],
   "source": [
    "# 4.d) Tune char LogisticRegression C\n",
    "char_C_grid = [0.5, 1.0, 2.0, 4.0]\n",
    "best_C = None\n",
    "best_f1_C = -1.0\n",
    "\n",
    "for C_val in char_C_grid:\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=3,\n",
    "            sublinear_tf=True,\n",
    "            max_features=200000,\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            C=C_val,\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        )),\n",
    "    ])\n",
    "    pipe.fit(X_train_text, y_train)\n",
    "    pred = pipe.predict(X_val_text)\n",
    "    f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    print(f\"C={C_val} -> macroF1={f1:.5f}\")\n",
    "    if f1 > best_f1_C:\n",
    "        best_f1_C = f1\n",
    "        best_C = C_val\n",
    "\n",
    "print(\"Best char C:\", best_C, \"with macroF1=\", round(best_f1_C, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa876b",
   "metadata": {},
   "source": [
    "### Step 3 – Define char‑level TF‑IDF + Logistic Regression pipeline\n",
    "\n",
    "**Aim:** Build a complementary character‑level model that can capture subword patterns (e.g. prefixes, suffixes, typos) using character n‑grams.\n",
    "\n",
    "- `TfidfVectorizer` works on `analyzer='char_wb'` with n‑grams `(3, 5)`, `min_df=3`, and up to 200k features.\n",
    "- The same Logistic Regression setup (`C=2.0`, `max_iter=2000`, `class_weight='balanced'`) is used as for the word model.\n",
    "\n",
    "**What the results show:**\n",
    "- Again no direct output; this defines `char_pipeline` which will be trained and evaluated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4350fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-only macroF1: 0.71779\n",
      "Char-only macroF1: 0.71417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "word_pipeline.fit(X_train_text, y_train)\n",
    "char_pipeline.fit(X_train_text, y_train)\n",
    "\n",
    "word_proba = word_pipeline.predict_proba(X_val_text)\n",
    "char_proba = char_pipeline.predict_proba(X_val_text)\n",
    "\n",
    "# For reference: each model alone\n",
    "pred_word = np.argmax(word_proba, axis=1)\n",
    "pred_char = np.argmax(char_proba, axis=1)\n",
    "\n",
    "print(\"Word-only macroF1:\", round(f1_score(y_val, pred_word, average=\"macro\"), 5))\n",
    "print(\"Char-only macroF1:\", round(f1_score(y_val, pred_char, average=\"macro\"), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe194026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_word=0.55, w_char=0.45 -> macroF1=0.72055\n",
      "w_word=0.64, w_char=0.36 -> macroF1=0.72057\n",
      "w_word=0.75, w_char=0.25 -> macroF1=0.72019\n",
      "w_word=0.84, w_char=0.16 -> macroF1=0.72058\n",
      "w_word=0.95, w_char=0.05 -> macroF1=0.71802\n",
      "Best weights: (0.84, 0.16) with macroF1= 0.72058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Try different weight combinations for word vs char\n",
    "weight_grid = [(0.55, 0.45), (0.64, 0.36), (0.75, 0.25), (0.84, 0.16), (0.95, 0.05)]\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_w = None\n",
    "\n",
    "for w_word, w_char in weight_grid:\n",
    "    final_proba = w_word * word_proba + w_char * char_proba\n",
    "    pred_ens = np.argmax(final_proba, axis=1)\n",
    "    f1 = f1_score(y_val, pred_ens, average=\"macro\")\n",
    "    print(f\"w_word={w_word:.2f}, w_char={w_char:.2f} -> macroF1={f1:.5f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_w = (w_word, w_char)\n",
    "\n",
    "print(\"Best weights:\", best_w, \"with macroF1=\", round(best_f1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4dd21",
   "metadata": {},
   "source": [
    "### Step 4 – Fit both models and compare word vs char performance\n",
    "\n",
    "**Aim:** Train the word and char pipelines on the training split and measure how each performs alone on the validation set using Macro F1.\n",
    "\n",
    "- Both pipelines are fitted on `X_train_text, y_train`.\n",
    "- `predict_proba` is used to obtain class probabilities on `X_val_text`, and `argmax` converts them to predicted labels.\n",
    "- Macro F1 scores are computed and printed separately for word‑only and char‑only models.\n",
    "\n",
    "**What the results show:**\n",
    "- `Word-only macroF1: ...` – performance of the word‑level model.\n",
    "- `Char-only macroF1: ...` – performance of the char‑level model.\n",
    "- These numbers tell you which representation is stronger on its own and set a baseline before ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3bb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL Ensemble macroF1 (source x2): 0.72063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Predict probabilities with source x2 text\n",
    "word_proba = word_pipeline.predict_proba(X_val_text)\n",
    "char_proba = char_pipeline.predict_proba(X_val_text)\n",
    "\n",
    "# Final ensemble (locked weights)\n",
    "final_proba = 0.7 * word_proba + 0.3 * char_proba\n",
    "pred_ens = np.argmax(final_proba, axis=1)\n",
    "\n",
    "f1_ens = f1_score(y_val, pred_ens, average=\"macro\")\n",
    "print(\"FINAL Ensemble macroF1 (source x2):\", round(f1_ens, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384184f",
   "metadata": {},
   "source": [
    "### Step 5 – Build a weighted word + char ensemble on validation\n",
    "\n",
    "**Aim:** Combine the word and char models into a single ensemble using fixed weights (0.7 for word, 0.3 for char) and evaluate it on the validation set.\n",
    "\n",
    "- Reuses `predict_proba` outputs for both models on `X_val_text`.\n",
    "- Forms `final_proba = 0.7 * word_proba + 0.3 * char_proba` and takes `argmax` to get ensemble predictions.\n",
    "- Computes Macro F1 for the ensemble and prints it.\n",
    "\n",
    "**What the results show:**\n",
    "- `FINAL Ensemble macroF1 (source x2): ...` – if this value is higher than both word‑only and char‑only scores, the ensemble is a true improvement.\n",
    "- Confirms that using both views of the text together is beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9700cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Both models trained on full training data\n"
     ]
    }
   ],
   "source": [
    "# Rebuild full text with source x2\n",
    "train_df[\"text\"] = build_text(train_df)\n",
    "\n",
    "X_full = train_df[\"text\"]\n",
    "y_full = train_df[\"label\"]\n",
    "\n",
    "# Train both models on ALL data\n",
    "word_pipeline.fit(X_full, y_full)\n",
    "char_pipeline.fit(X_full, y_full)\n",
    "\n",
    "print(\"✅ Both models trained on full training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca766b",
   "metadata": {},
   "source": [
    "### Step 6 – Retrain both models on all labeled data\n",
    "\n",
    "**Aim:** After validating the ensemble, refit the word and char pipelines on the **full** training set so they can use every labeled example before generating submissions.\n",
    "\n",
    "- Rebuilds `text` for all rows using `build_text` (with `source` repeated).\n",
    "- Sets `X_full` and `y_full` and fits both pipelines on the entire dataset.\n",
    "\n",
    "**What the results show:**\n",
    "- The message `✅ Both models trained on full training data` confirms that training completed successfully on all labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8620861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval samples: 20000\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv(\"evaluation.csv\")\n",
    "eval_df[\"text\"] = build_text(eval_df)\n",
    "\n",
    "X_eval = eval_df[\"text\"]\n",
    "\n",
    "print(\"Eval samples:\", len(X_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817e41a",
   "metadata": {},
   "source": [
    "### Step 7 – Prepare evaluation text\n",
    "\n",
    "**Aim:** Load the competition evaluation set and build its `text` field in exactly the same way as for the training data, so the pipelines can be applied consistently.\n",
    "\n",
    "- Reads `evaluation.csv` into `eval_df`.\n",
    "- Applies `build_text` to create `eval_df[\"text\"]`.\n",
    "- Extracts `X_eval` as the text Series and prints the number of evaluation samples.\n",
    "\n",
    "**What the results show:**\n",
    "- `Eval samples: ...` confirms that the evaluation file was loaded correctly and that the number of rows matches expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "345fb0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission_ensemble_tuned.csv created\n",
      "   Id  Predicted\n",
      "0   0          5\n",
      "1   1          2\n",
      "2   2          5\n",
      "3   3          1\n",
      "4   4          5\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "word_proba_test = word_pipeline.predict_proba(X_eval)\n",
    "char_proba_test = char_pipeline.predict_proba(X_eval)\n",
    "\n",
    "# Ensemble\n",
    "final_proba_test = 0.7 * word_proba_test + 0.3 * char_proba_test\n",
    "preds_test = np.argmax(final_proba_test, axis=1)\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": eval_df[\"Id\"],\n",
    "    \"Predicted\": preds_test\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_ensemble_tuned.csv\", index=False)\n",
    "\n",
    "print(\"✅ submission_ensemble_tuned.csv created\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d863f6d",
   "metadata": {},
   "source": [
    "### Step 8 – Create tuned ensemble submission file\n",
    "\n",
    "**Aim:** Apply the trained word + char ensemble to the evaluation set and save the main submission file `submission_ensemble_tuned.csv`.\n",
    "\n",
    "- Computes `predict_proba` for both models on `X_eval`.\n",
    "- Forms the same 0.7/0.3 ensemble and takes `argmax` to get final predictions.\n",
    "- Builds a DataFrame with `Id` and `Predicted` and writes it to CSV.\n",
    "\n",
    "**What the results show:**\n",
    "- The message `✅ submission_ensemble_tuned.csv created` confirms the file was written.\n",
    "- `submission.head()` lets you visually inspect the first few rows (Ids and predicted labels) to sanity‑check the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cef3e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote submission_ensemble_src1.csv\n",
      "✅ wrote submission_ensemble_src2.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_text_src1(df):\n",
    "    return df[\"source\"].fillna(\"\") + \" \" + df[\"title\"].fillna(\"\") + \" \" + df[\"article\"].fillna(\"\")\n",
    "\n",
    "def build_text_src2(df):\n",
    "    src = df[\"source\"].fillna(\"\")\n",
    "    return (src + \" \") * 2 + df[\"title\"].fillna(\"\") + \" \" + df[\"article\"].fillna(\"\")\n",
    "\n",
    "def make_submission(text_builder, out_name):\n",
    "    train_df = pd.read_csv(\"development.csv\")\n",
    "    eval_df  = pd.read_csv(\"evaluation.csv\")\n",
    "\n",
    "    X_full = text_builder(train_df)\n",
    "    y_full = train_df[\"label\"]\n",
    "    X_eval = text_builder(eval_df)\n",
    "\n",
    "    # fit full\n",
    "    word_pipeline.fit(X_full, y_full)\n",
    "    char_pipeline.fit(X_full, y_full)\n",
    "\n",
    "    # predict\n",
    "    wp = word_pipeline.predict_proba(X_eval)\n",
    "    cp = char_pipeline.predict_proba(X_eval)\n",
    "    preds = np.argmax(0.7*wp + 0.3*cp, axis=1)\n",
    "\n",
    "    sub = pd.DataFrame({\"Id\": eval_df[\"Id\"], \"Predicted\": preds})\n",
    "    sub.to_csv(out_name, index=False)\n",
    "    print(\"✅ wrote\", out_name)\n",
    "\n",
    "make_submission(build_text_src1, \"submission_ensemble_src1.csv\")\n",
    "make_submission(build_text_src2, \"submission_ensemble_src2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11527a",
   "metadata": {},
   "source": [
    "### Step 9 – Generate alternative submissions with different text recipes\n",
    "\n",
    "**Aim:** Produce two extra submission files using slightly different ways of building the text field from `source`, `title`, and `article`, while keeping the same word + char ensemble.\n",
    "\n",
    "- `build_text_src1` uses `source` once; `build_text_src2` repeats `source` twice, similar to earlier.\n",
    "- `make_submission` is a helper that, for a given text builder, fits both pipelines on all training data, predicts on evaluation text, ensembles with weights (0.7, 0.3), and writes a CSV.\n",
    "- Two submissions are written: `submission_ensemble_src1.csv` and `submission_ensemble_src2.csv`.\n",
    "\n",
    "**What the results show:**\n",
    "- Lines like `✅ wrote submission_ensemble_src1.csv` confirm each file was created.\n",
    "- These variants let you compare or ensemble across slightly different input formulations if desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
